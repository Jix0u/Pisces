# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T90tfT70uDClUQaCpfMsJ2yi0GnxIskN
"""

!wget http://hog.ee.columbia.edu/craffel/lmd/lmd_full.tar.gz

# !rm -r /content/lmd_full.tar.gz
!ls

!tar -xvf /content/lmd_full.tar.gz

!git clone https://github.com/Tegridy-Code/tegridy-tools
!pip install tqdm

print('Loading needed modules. Please wait...')

import os
from datetime import datetime
import secrets
import copy
import tqdm
from tqdm import tqdm
import re

# Create IO dirs...

print('Creating IO dirs...')

if not os.path.exists('Output'):
    os.mkdir('Output')

if not os.path.exists('In'):
    os.mkdir('In')
if not os.path.exists('Out'):
    os.mkdir('Out')

print('Done!')

os.chdir('./tegridy-tools/tegridy-tools')

print('Loading TMIDIX module...')
import TMIDIX

os.chdir('./')

#@title Process MIDIs

sorted_or_random_file_loading_order = True


print('TMIDIX MIDI Processor')
print('Starting up...')
###########

files_count = 0

gfiles = []

melody_chords_f = []

###########

print('Loading MIDI files...')
print('This may take a while on a large dataset in particular.')

dataset_addr = "lmd_full"
# os.chdir(dataset_addr)
filez = list()
for (dirpath, dirnames, filenames) in os.walk(dataset_addr):
    filez += [os.path.join(dirpath, file) for file in filenames]
print('=' * 70)

if filez == []:
  print('Could not find any MIDI files. Please check Dataset dir...')
  print('=' * 70)

if sorted_or_random_file_loading_order:
  print('Sorting files...')
  filez.sort()
  print('Done!')
  print('=' * 70)

stats = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

v = 1
print('Processing MIDI files. Please wait...')
for f in tqdm(filez):
  try:
    fn = os.path.basename(f)
    fn1 = fn.split('.')[0]

    files_count += 1



    if not os.path.exists('./Output/'+fn):



        #print('Loading MIDI file...')
        score = TMIDIX.midi2score(open(f, 'rb').read())

        events_matrix = []

        itrack = 1

        patches = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

        patch_map = [[0, 1, 2, 3, 4, 5, 6, 7], # Piano
                     [24, 25, 26, 27, 28, 29, 30], # Guitar
                     [32, 33, 34, 35, 36, 37, 38, 39], # Bass
                     [40, 41], # Violin
                     [42, 43], # Cello
                     [46], # Harp
                     [56, 57, 58, 59, 60], # Trumpet
                     [71, 72], # Clarinet
                     [73, 74, 75], # Flute
                     [-1], # Fake Drums
                     [52, 53] # Choir
                    ]

        while itrack < len(score):
            for event in score[itrack]:
                if event[0] == 'note' or event[0] == 'patch_change':
                    events_matrix.append(event)
            itrack += 1

        events_matrix1 = []
        for event in events_matrix:
                if event[0] == 'patch_change':
                    patches[event[2]] = event[3]

                if event[0] == 'note':
                    event.extend([patches[event[3]]])
                    once = False
                    for p in patch_map:
                        if event[6] in p and event[3] != 9: # Except the drums
                            event[3] = patch_map.index(p)
                            once = True
                    if not once and event[3] != 9: # Except the drums
                        event[3] = 11 # All other instruments/patches channel
                    if event[3] < 11: # We won't write all other instruments for now...
                        events_matrix1.append(event)
                        stats[event[3]] += 1

        events_matrix1.sort()

        detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(events_matrix1,
                                                               number_of_ticks_per_quarter=score[0],
                                                               output_file_name='./Output/'+fn1,
                                                              output_signature='Project Los Angeles',
                                                              track_name='Tegridy Code 2022',
                                                              list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 52, 0, 0, 0, 0, 0]
                                                              )

        gfiles.append(f)

  except KeyboardInterrupt:
    print('Saving current progress and quitting...')
    break

  except:
    print('Bad MIDI:', f)
    continue

import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam

from typing import Dict, List, Optional, Sequence, Tuple
import collections
import datetime
import glob
import numpy as np
import pretty_midi
import pathlib
import pandas as pd

from IPython import display
from matplotlib import pyplot as plt
import seaborn as sns

seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)

# Sampling rate for audio playback
_SAMPLING_RATE = 16000

data_dir = pathlib.Path('lmd_full')

filenames = glob.glob(str(data_dir/'**/*.mid*'))
print('Number of files:', len(filenames))

!pip install pretty_midi
import pretty_midi
sample_file = filenames[2]
print(sample_file)
pm = pretty_midi.PrettyMIDI(sample_file)

print('Number of instruments:', len(pm.instruments))

# Iterate over each instrument
for i, instrument in enumerate(pm.instruments):
    instrument_name = pretty_midi.program_to_instrument_name(instrument.program)
    print(f'\nInstrument {i + 1}: {instrument_name}')

    # Extracting the notes for the current instrument
    print('Number of notes:', len(instrument.notes))
    for j, note in enumerate(instrument.notes[:10]):
        note_name = pretty_midi.note_number_to_name(note.pitch)
        duration = note.end - note.start
        print(f'  Note {j + 1}: pitch={note.pitch}, note_name={note_name}, duration={duration:.4f}')

!ls lmd_full.tar.gz/

def midi_to_notes(midi_file: str, instrument) -> pd.DataFrame:
    pm = pretty_midi.PrettyMIDI(midi_file)
    notes = collections.defaultdict(list)
    # Sort the notes by start time
    sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
    prev_start = sorted_notes[0].start

    for note in sorted_notes:
        start = note.start
        end = note.end
        notes['pitch'].append(note.pitch)
        notes['start'].append(start)
        notes['end'].append(end)
        notes['step'].append(start - prev_start)
        notes['duration'].append(end - start)
        prev_start = start

    return pd.DataFrame({name: np.array(value) for name, value in notes.items()})
all_raw_notes = []
for i, instrument in enumerate(pm.instruments):
    raw_notes = midi_to_notes(sample_file, instrument)
    all_raw_notes.append(raw_notes)


print(all_raw_notes)

def notes_to_midi(notes: pd.DataFrame, out_file: str, instrument_name: str,
                  velocity: int = 100) -> pretty_midi.PrettyMIDI:

    pm = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(
      program=pretty_midi.instrument_name_to_program(
          instrument_name))

    prev_start = 0
    for i, note in notes.iterrows():
        start = float(prev_start + note['step'])
        end = float(start + note['duration'])

        note = pretty_midi.Note(velocity=velocity, pitch=int(note['pitch']),
                                start=start, end=end)
        instrument.notes.append(note)
        prev_start = start

    pm.instruments.append(instrument)
    pm.write(out_file)
    return pm

# %%
def create_sequences(dataset: tf.data.Dataset, seq_length: int,
                     vocab_size = 128) -> tf.data.Dataset:
    """Returns TF Dataset of sequence and label examples."""
    seq_length = seq_length+1

    # Take 1 extra for the labels
    windows = dataset.window(seq_length, shift=1, stride=1,
                              drop_remainder=True)

    # `flat_map` flattens the" dataset of datasets" into a dataset of tensors
    flatten = lambda x: x.batch(seq_length, drop_remainder=True)
    sequences = windows.flat_map(flatten)

    # Normalize note pitch
    def scale_pitch(x):
        x = x/[vocab_size,1.0,1.0]
        return x

    # Split the labels
    def split_labels(sequences):
        inputs = sequences[:-1]
        labels_dense = sequences[-1]
        labels = {key:labels_dense[i] for i,key in enumerate(key_order)}

        return scale_pitch(inputs), labels

    return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)
  # %%
def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):
    mse = (y_true - y_pred) ** 2
    positive_pressure = 10 * tf.maximum(-y_pred, 0.0)
    return tf.reduce_mean(mse + positive_pressure)

num_files = 5
all_notes = []
for f in filenames[:num_files]:
  for i, instrument in enumerate(pm.instruments):
    notes = midi_to_notes(sample_file, instrument)
    all_notes.append(notes)

all_notes = pd.concat(all_notes)

key_order = ['pitch', 'step', 'duration']
train_notes = np.stack([all_notes[key] for key in key_order], axis=1)

notes_ds = tf.data.Dataset.from_tensor_slices(train_notes)
notes_ds.element_spec

def predict_next_note(notes: np.ndarray, keras_model: tf.keras.Model,
                      temperature: float = 1.0) -> int:
    """Generates a note IDs using a trained sequence model."""

    assert temperature > 0

    # Add batch dimension
    inputs = tf.expand_dims(notes, 0)

    predictions = model.predict(inputs)
    pitch_logits = predictions['pitch']
    step = predictions['step']
    duration = predictions['duration']

    pitch_logits /= temperature
    pitch = tf.random.categorical(pitch_logits, num_samples=1)
    pitch = tf.squeeze(pitch, axis=-1)
    duration = tf.squeeze(duration, axis=-1)
    step = tf.squeeze(step, axis=-1)

    # `step` and `duration` values should be non-negative
    step = tf.maximum(0, step)
    duration = tf.maximum(0, duration)

    return int(pitch), float(step), float(duration)

n_notes = len(all_notes)
print('Number of notes parsed:', n_notes)
seq_length = 25
vocab_size = 128
seq_ds = create_sequences(notes_ds, seq_length, vocab_size)

# %%
batch_size = 64
buffer_size = n_notes - seq_length  # the number of items in the dataset
train_ds = (seq_ds
            .shuffle(buffer_size)
            .batch(batch_size, drop_remainder=True)
            .cache()
            .prefetch(tf.data.experimental.AUTOTUNE))

# %%
input_shape = (25, 3)
learning_rate = 0.005

inputs = Input(input_shape)
x = LSTM(128)(inputs)

outputs = {'pitch': Dense(128, name='pitch')(x),
           'step': Dense(1, name='step')(x),
           'duration': Dense(1, name='duration')(x),
          }

model = Model(inputs, outputs)

loss = {'pitch': SparseCategoricalCrossentropy(from_logits=True),
        'step': mse_with_positive_pressure,
        'duration': mse_with_positive_pressure,
       }

optimizer = Adam(learning_rate=learning_rate)

model.compile(loss=loss, optimizer=optimizer)

model.summary()

# %%
# Creating the necessary callbacks

callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./training_checkpoints/ckpt_{epoch}', save_weights_only=True),
             tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5,
                                              verbose=1, restore_best_weights=True),]

# %%
model.compile(loss = loss,
              loss_weights = {'pitch': 0.05, 'step': 1.0, 'duration':1.0,},
              optimizer = optimizer)

epochs = 50

history = model.fit(train_ds,
                    epochs=epochs,
                    callbacks=callbacks,)

temperature = 2.0
num_predictions = 120

sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)

# The initial sequence of notes while the pitch is normalized similar to training sequences
input_notes = sample_notes[:seq_length] / np.array([vocab_size, 1, 1])

# Pad input_notes to ensure it has the correct sequence length
target_seq_length = 25
if len(input_notes) < target_seq_length:
    pad_length = target_seq_length - len(input_notes)
    input_notes = np.pad(input_notes, ((pad_length, 0), (0, 0)), mode='constant')

generated_notes = []
prev_start = 0

for _ in range(num_predictions):
    pitch, step, duration = predict_next_note(input_notes, model, temperature)
    start = prev_start + step
    end = start + duration
    input_note = (pitch, step, duration)
    generated_notes.append((*input_note, start, end))
    input_notes = np.delete(input_notes, 0, axis=0)
    input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)
    prev_start = start

generated_notes = pd.DataFrame(
    generated_notes, columns=(*key_order, 'start', 'end'))

generated_notes.head(10)

def combine_midi_files(midi_files, combined_out_file):
    combined_midi = MidiFile()
    for midi_file_path in midi_files:
        midi = MidiFile(midi_file_path)
        for track in midi.tracks:
            new_track = MidiTrack()
            for msg in track:
                new_track.append(msg)
            combined_midi.tracks.append(new_track)

    combined_midi.save(combined_out_file)
    return combined_out_file

def split_dataframe(dataframe, num_splits):
    # Calculate the number of rows per split
    rows_per_split = len(dataframe) // num_splits

    # Initialize a list to hold the split DataFrames
    split_dataframes = []

    # Split the DataFrame into chunks and append them to the list
    for i in range(num_splits):
        start_index = i * rows_per_split
        end_index = start_index + rows_per_split
        if i == num_splits - 1:  # Adjust the end index for the last split
            end_index = len(dataframe)
        split_dataframes.append(dataframe.iloc[start_index:end_index])

    return split_dataframes

def out_folder(generated_notes, out_folder, instrument_names):
    if not os.path.exists(out_folder):
        os.makedirs(out_folder)

    midi_files = []
    for i, instrument_name in enumerate(instrument_names):
        out_file = os.path.join(out_folder, f'instrument_{i}.midi')
        out_pm = notes_to_midi(generated_notes[i], out_file=out_file, instrument_name=instrument_name)
        midi_files.append(out_file)

    # Combine MIDI files into one
    combined_out_file = os.path.join(out_folder, 'combined_output.midi')
    combined_pm = combine_midi_files(midi_files, combined_out_file)

    return combined_out_file

instrument_names = ['Timpani', 'Steel Drums', 'Voice Oohs']
hi = []
bye = []

for instrument in instrument_names:
  out_file = str(instrument)+'instrument.midi'
  out_pm = notes_to_midi(generated_notes, out_file=out_file, instrument_name=instrument_name)
  hi.append(out_pm)
  bye.append(out_file)

from google.colab import files

# Save the MIDI file to the Colab working directory
for i, out_file in enumerate(bye):
  hi[i].write(out_file)
  files.download(out_file)

# Download the MIDI file

